{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (B)uild and (T)rain tutorial on CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from keras import backend as keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from CNN_utils import *\n",
    "from Cifar_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8,8) # Make the figures a bit bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data (~160MB)\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataset:\n",
    "print('Size of training data: '+str(trainX.shape))\n",
    "print('Labels in training data: '+str(trainY.shape))\n",
    "print('   ')\n",
    "print('Size of test data: '+str(testX.shape))\n",
    "print('Labels in test data: '+str(testY.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up dataset classes\n",
    "classes = ['airplane',\n",
    "  'automobile',\n",
    "  'bird',\n",
    "  'cat',\n",
    "  'deer',\n",
    "  'dog',\n",
    "  'frog',\n",
    "  'horse',\n",
    "  'ship',\n",
    "  'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the training and testing labels\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-up batch generation and image preprocessing\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# batch generator with data augmentation\n",
    "#train_datagen = ImageDataGenerator(\n",
    "#        rescale=1./255,\n",
    "#        shear_range=0.2,\n",
    "#        zoom_range=0.2,\n",
    "#        horizontal_flip=True)\n",
    "\n",
    "train_set = train_datagen.flow(\n",
    "        trainX, trainY,\n",
    "        batch_size=16)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow(\n",
    "        testX, testY,\n",
    "        batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    my_random = i+np.random.randint(100)\n",
    "    plt.imshow(trainX[my_random,::], interpolation='none') #, cmap='gray'\n",
    "    temp_class = np.argmax(trainY[my_random,::])\n",
    "    plt.title(\"Class: {}\".format(classes[temp_class]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "## Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (32,32,3)\n",
    "#initial layer\n",
    "inputs = Input(input_size)\n",
    "\n",
    "# block 1 (filters with size 32)\n",
    "x = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "x = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# block 2 (filters with size 64)\n",
    "\n",
    "\n",
    "\n",
    "# block 3 (filters with size 128)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# final block \n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "out = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model preparation\n",
    "adamopt = Adam(lr=1e-4, decay=1e-6) #Adam(lr=1e-6, decay=1e-8)\n",
    "model.compile(optimizer=adamopt, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(\n",
    "        train_set,\n",
    "        steps_per_epoch=500,\n",
    "        epochs=5,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_learning_acc(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "_, acc = model.evaluate(testX, testY, verbose=1)\n",
    "print('Model accuracy -> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "We set the image from test_set as input to the network and get predictions from the output layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference on random examples\n",
    "print(\"Inference results: (prediction/ground_truth)\")\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    my_random = i+np.random.randint(100)\n",
    "    im = testX[my_random,::]\n",
    "    plt.imshow(im, interpolation='none') #, cmap='gray'\n",
    "    \n",
    "    x = image.img_to_array(im)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    preds = model.predict(x)\n",
    "\n",
    "    temp_class = np.argmax(preds[0])\n",
    "    \n",
    "    title_string = classes[temp_class] + \"/\" + classes[np.argmax(testY[my_random])]\n",
    "    plt.title(title_string)\n",
    "    \n",
    "    #plt.title(\"Class {}\".format(classes[temp_class]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on specified example\n",
    "idx = 5 #54\n",
    "\n",
    "\n",
    "im = testX[idx,::]\n",
    "plt.imshow(im)\n",
    "\n",
    "x = image.img_to_array(im)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "preds = model.predict(x)\n",
    "\n",
    "temp_class = np.argmax(preds[0])\n",
    "title_string = \"Predicted: \"+classes[temp_class] + \", GT: \" + classes[np.argmax(testY[idx])]\n",
    "plt.title(title_string)\n",
    "\n",
    "print('Probaility for all classes:')\n",
    "for i in range(10):\n",
    "    print(\"%2.3f\"% (preds[0][i]),' : ',classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Pretrained networks are a good way to start your project as they are usually trained on large amounts of data and using resources that arenâ€™t usually available to everyone. Based on the feature maps inside CNN you can see the focus (attention) of the network as well as perform coarse localisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
