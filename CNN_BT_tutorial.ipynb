{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1P6o_mYdKRa"
      },
      "source": [
        "# CNN (B)uild and (T)rain tutorial on CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG_47C_EdKRg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from keras import backend as keras\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from CNN_utils import *\n",
        "from Cifar_utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS0MKuQrdKRj"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = (8,8) # Make the figures a bit bigger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBcqSFeMdKRk"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVDB4pb4dKRk"
      },
      "outputs": [],
      "source": [
        "#Load data (~160MB)\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyl1eXV5dKRl"
      },
      "outputs": [],
      "source": [
        "# Inspect the dataset:\n",
        "print('Size of training data: '+str(trainX.shape))\n",
        "print('Labels in training data: '+str(trainY.shape))\n",
        "print('   ')\n",
        "print('Size of test data: '+str(testX.shape))\n",
        "print('Labels in test data: '+str(testY.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6oZ2NogdKRl"
      },
      "outputs": [],
      "source": [
        "# Set-up dataset classes\n",
        "classes = ['airplane',\n",
        "  'automobile',\n",
        "  'bird',\n",
        "  'cat',\n",
        "  'deer',\n",
        "  'dog',\n",
        "  'frog',\n",
        "  'horse',\n",
        "  'ship',\n",
        "  'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBNkWs8IdKRn"
      },
      "outputs": [],
      "source": [
        "# one-hot encode the training and testing labels\n",
        "trainY = to_categorical(trainY, 10)\n",
        "testY = to_categorical(testY, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRGw2ZCgdKRo"
      },
      "outputs": [],
      "source": [
        "#set-up batch generation and image preprocessing\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# batch generator with data augmentation\n",
        "#train_datagen = ImageDataGenerator(\n",
        "#        rescale=1./255,\n",
        "#        shear_range=0.2,\n",
        "#        zoom_range=0.2,\n",
        "#        horizontal_flip=True)\n",
        "\n",
        "train_set = train_datagen.flow(\n",
        "        trainX, trainY,\n",
        "        batch_size=16)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_set = test_datagen.flow(\n",
        "        testX, testY,\n",
        "        batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX6tI5X6dKRp"
      },
      "source": [
        "Let's look at some examples of the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-UFL_ebdKRp"
      },
      "outputs": [],
      "source": [
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.axis('off')\n",
        "    my_random = i+np.random.randint(100)\n",
        "    plt.imshow(trainX[my_random,::], interpolation='none') #, cmap='gray'\n",
        "    temp_class = np.argmax(trainY[my_random,::])\n",
        "    plt.title(\"Class: {}\".format(classes[temp_class]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8dPcZvrdKRq"
      },
      "source": [
        "# CNN\n",
        "\n",
        "## Building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-bLQPHGdKRr"
      },
      "outputs": [],
      "source": [
        "input_size = (32,32,3)\n",
        "#initial layer\n",
        "inputs = Input(input_size)\n",
        "\n",
        "# block 1 (filters with size 32)\n",
        "x = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "x = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# block 2 (filters with size 64)\n",
        "\n",
        "\n",
        "\n",
        "# block 3 (filters with size 128)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# final block \n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "out = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsSVOK7DdKRr"
      },
      "outputs": [],
      "source": [
        "# Model preparation\n",
        "adamopt = Adam(lr=1e-4, decay=1e-6) #Adam(lr=1e-6, decay=1e-8)\n",
        "model.compile(optimizer=adamopt, loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7YhOrGmdKRr"
      },
      "source": [
        "## Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_9zHRIVdKRs"
      },
      "outputs": [],
      "source": [
        "hist = model.fit_generator(\n",
        "        train_set,\n",
        "        steps_per_epoch=100,\n",
        "        epochs=5,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB9kCGsmdKRs"
      },
      "outputs": [],
      "source": [
        "print_learning_acc(hist.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZEesQStdKRs"
      },
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "_, acc = model.evaluate(testX, testY, verbose=1)\n",
        "print('Model accuracy -> %.3f' % (acc * 100.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL9qNXA8dKRt"
      },
      "source": [
        "## Inference\n",
        "We set the image from test_set as input to the network and get predictions from the output layer of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAl5bU3GdKRt"
      },
      "outputs": [],
      "source": [
        "#Inference on random examples\n",
        "print(\"Inference results: (prediction/ground_truth)\")\n",
        "for i in range(15):\n",
        "    plt.subplot(3,5,i+1)\n",
        "    plt.axis('off')\n",
        "    my_random = i+np.random.randint(100)\n",
        "    im = testX[my_random,::]\n",
        "    plt.imshow(im, interpolation='none') #, cmap='gray'\n",
        "    \n",
        "    x = img_to_array(im)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    preds = model.predict(x)\n",
        "\n",
        "    temp_class = np.argmax(preds[0])\n",
        "    \n",
        "    title_string = classes[temp_class] + \"/\" + classes[np.argmax(testY[my_random])]\n",
        "    plt.title(title_string)\n",
        "    \n",
        "    #plt.title(\"Class {}\".format(classes[temp_class]))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CguCWzvdKRt"
      },
      "outputs": [],
      "source": [
        "# inference on specified example\n",
        "idx = 5 #54\n",
        "\n",
        "\n",
        "im = testX[idx,::]\n",
        "plt.imshow(im)\n",
        "\n",
        "x = img_to_array(im)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "preds = model.predict(x)\n",
        "\n",
        "temp_class = np.argmax(preds[0])\n",
        "title_string = \"Predicted: \"+classes[temp_class] + \", GT: \" + classes[np.argmax(testY[idx])]\n",
        "plt.title(title_string)\n",
        "\n",
        "print('Probaility for all classes:')\n",
        "for i in range(10):\n",
        "    print(\"%2.3f\"% (preds[0][i]),' : ',classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkTKLftsdKRt"
      },
      "source": [
        "## Summary\n",
        "One of the main problems of deep neural networks is 'overfitting'. It leads to poor adaptation of the model to new data and shows as high variance in the train/dev set error comparison. \n",
        " To reduce this problem techniques like regularization, data augmentation, early stopping and dropout could be used. Expansion of the model also tends to lower the overfitting problem but increase the amount of computational resources needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sh66MYodKRu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}