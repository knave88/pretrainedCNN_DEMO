{"cells":[{"cell_type":"markdown","metadata":{"id":"mNNKSYULyd8r"},"source":["# Image classification wth CNN pretrained on imagenet database"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE0xXr5xyd8x"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import matplotlib.patches as patches\n","\n","from keras import backend as keras\n","from keras.layers import *\n","from keras.models import Model\n","\n","from keras.applications.mobilenet import MobileNet\n","from keras.applications.mobilenet import preprocess_input, decode_predictions\n","\n","#from keras.preprocessing import image\n","#from keras.preprocessing.image import load_img\n","from tensorflow.keras.utils import load_img\n","from tensorflow.keras.utils import img_to_array\n","\n","\n","from keras import backend as K\n","import tensorflow as tf\n","\n","#from CNN_utils import *"]},{"cell_type":"code","source":["#!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/CNN_utils.py' -O CNN_utils.py\n","\n","!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/cat.1.jpg' -O cat.1.jpg\n","!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/cat.0000.jpg' -O cat.0000.jpg\n","!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/cat.00000.jpg' -O cat.00000.jpg\n","!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/dog.1018.jpg' -O dog.1018.jpg\n"],"metadata":{"id":"4J2n-j67y3J2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/CNN_utils.py' -O CNN_utils.py\n","from CNN_utils import *"],"metadata":{"id":"I3j9TTKunZ6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAH4_WgYyd8z"},"outputs":[],"source":["plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger"]},{"cell_type":"markdown","metadata":{"id":"5qRj0M6Yyd80"},"source":["## Load pretrained CNN\n","We'll use Moblilenet as it is one of the lightweight models. Depthwise Separable Convolution is used to reduce the model size and complexity.\n","We load a MobileNet model, with weights pre-trained on ImageNet. The default input size for this model is 224x224. The network outputs the probability for 1000 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Z6cc3ufryd81"},"outputs":[],"source":["base_model = MobileNet(include_top=True, weights='imagenet') #classes=1000\n","#base_model.summary()\n","print(\"Total parameters: \"+str(base_model.count_params())+\" in \"+str(len(base_model.layers))+\" layers\" )"]},{"cell_type":"markdown","metadata":{"id":"LtNeHuPOyd81"},"source":["<img src=\"https://github.com/knave88/pretrainedCNN_DEMO/blob/main/mobilenet.png?raw=1\">"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"YBa4FGBJyd82"},"outputs":[],"source":["# mark the convolution layer for feature extraction\n","layer_index = 'conv_pw_13'\n","model = Model(inputs=base_model.input, outputs=(base_model.get_layer(layer_index).output,base_model.layers[-1].output))"]},{"cell_type":"markdown","metadata":{"id":"9n11DBWkyd83"},"source":["## Load cat image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntmEOYAByd83"},"outputs":[],"source":["img_path = 'cat.1.jpg'\n","img = load_img(img_path, target_size=(224, 224))\n","\n","plt.figure()\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"nlWrFAe2yd84"},"source":["## Inference\n","We set the image above as input to the network and get predictions from the output layer of the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13D0ZQ2Fyd85"},"outputs":[],"source":["x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","conv_out, preds = model.predict(x)\n","# decode the results into a list of tuples (class, description, probability)\n","# (one such list for each sample in the batch)\n","decode_preds = decode_predictions(preds, top=13)\n","print('Top 3 predictions:', decode_preds[0][0][1], decode_preds[0][1][1], decode_preds[0][2][1])\n"]},{"cell_type":"markdown","metadata":{"id":"qkTRal_gyd86"},"source":["## Load dog image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMLbsKwRyd86"},"outputs":[],"source":["img_path = 'dog.1018.jpg'\n","img = load_img(img_path, target_size=(224, 224))\n","\n","plt.figure()\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"R2kOmXj5yd87"},"source":["## Inference\n","We set the image above as input to the network and get predictions from the output layer of the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-rZgnBFyd87"},"outputs":[],"source":["x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","conv_out, preds = model.predict(x)\n","# decode the results into a list of tuples (class, description, probability)\n","# (one such list for each sample in the batch)\n","decode_preds = decode_predictions(preds, top=13)\n","print('Top 3 predictions:', decode_preds[0][0][1], decode_preds[0][1][1], decode_preds[0][2][1])\n"]},{"cell_type":"markdown","metadata":{"id":"w-2mWXI4yd88"},"source":["## More sophisticated example"]},{"cell_type":"markdown","metadata":{"id":"EKSZc_Qsyd88"},"source":["First, we'll begin with simple enough image of a cat."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9ZR2Hfjyd88"},"outputs":[],"source":["img_path = 'cat.0000.jpg'\n","img = load_img(img_path, target_size=(224, 224))\n","x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","conv_out, preds = model.predict(x)\n","decode_preds = decode_predictions(preds, top=13)\n","\n","plt.figure()\n","plt.imshow(img)\n","print('Top prediction: '+str(decode_preds[0][0][1]));"]},{"cell_type":"markdown","metadata":{"id":"w1YOMQITyd89"},"source":["But what if the animal was not the only object present in the image?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pvJLzgZ2yd89"},"outputs":[],"source":["img_path = 'cat.00000.jpg'\n","img = load_img(img_path, target_size=(224, 224))\n","\n","plt.figure()\n","plt.imshow(img)\n","\n","x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","conv_out, preds = model.predict(x)\n","# decode the results into a list of tuples (class, description, probability)\n","# (one such list for each sample in the batch)\n","decode_preds = decode_predictions(preds, top=13)\n","print('Top 3 predictions:', decode_preds[0][0][1], decode_preds[0][1][1], decode_preds[0][2][1])"]},{"cell_type":"markdown","metadata":{"id":"KYVvhfyuyd89"},"source":["As the cat is not in the top 3, let's see further down the list of top predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UM07tarfyd8-"},"outputs":[],"source":["decode_preds[0]"]},{"cell_type":"markdown","metadata":{"id":"GmJPpOYZyd8-"},"source":["There it is: No 8 'Egyptian_cat' class with probability of 0.014584725.\n","\n","Since this network has 1000 classes it focuses on other objects present in the image."]},{"cell_type":"markdown","metadata":{"id":"D0Mbyx0nyd8-"},"source":["# Weakly supervised object localisation\n","\n","Weakly-supervised learning provides a way to omit difficult process of pixel-level annotation of data. Models trained through WSL exploit unlabeled inputs, as well as coarse or ambiguous labels.\n","\n","The goal of such approach is to classify images with only global image-level labels, while producing pixel-level label predictions, thereby localizing the important regions of interest that are linked to the model's global decision. Pinpointing image sub-regions that were used by the model to make its global image-class prediction not only provides weakly supervised segmentation, but also enables interpretable deep-network classifiers."]},{"cell_type":"markdown","metadata":{"id":"Y_j0Pak4yd8-"},"source":["## Heat map for top predictions\n","Let's take a look at the feature map from last convoluton layer to see the area on which the prediction was made."]},{"cell_type":"code","source":["def get_bounds(out, percentile=95):\n","    # Get bounding box of 95+ percentile pixels\n","    a = out.flatten()\n","    filtered = np.array([1 if x > np.percentile(a, percentile) else 0 for x in a]).reshape(224,224)\n","    left, up, down, right = 224, 224, 0, 0\n","    for x in range(224):\n","        for y in range(224):\n","            if filtered[y,x] == 1:\n","                left = min(left, x)\n","                right = max(right, x)\n","                up = min(up, y)\n","                down = max(down, y)\n","    return left, up, down, right\n","\n","def heatmap_for_top_pred(img2infer, model_CAM,  pred_index=None, figsizeX=12, analysed_preds=7):\n","    img = load_img(img2infer, target_size=(224, 224))\n","    x = img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = model_CAM(x)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","\n","    heatmap = cv2.resize(heatmap.numpy(), (224,224))\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    left, up, down, right = get_bounds(heatmap, percentile=95)\n","\n","    rect = patches.Rectangle((left, up), (right-left), (down-up), linewidth=1,  edgecolor='r', facecolor='none')\n","\n","    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(figsizeX, figsizeX))\n","    axes.imshow(img, alpha=0.7)\n","    axes.imshow(heatmap, cmap='jet', alpha=0.3)\n","    left, up, down, right = get_bounds(heatmap, percentile=95)\n","    rect = patches.Rectangle((left, up), (right-left), (down-up), linewidth=1,  edgecolor='r', facecolor='none')\n","    axes.add_patch(rect)\n","    axes.set_title('Heat map and bounding box for prediction: '+str(decode_preds[0][analysed_preds][1]))\n","    return None"],"metadata":{"id":"ORQDK5M91BTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sprJNKb-yd8_"},"outputs":[],"source":["heatmap_for_top_pred ('cat.1.jpg', model) #extract_layer_name='conv_pw_13'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVhf9-Wlyd8_"},"outputs":[],"source":["heatmap_for_top_pred ('dog.1018.jpg', model)"]},{"cell_type":"markdown","metadata":{"id":"-Ldysxhzyd8_"},"source":["Here we can see based on which part of image the predictions were made.\n","As you can see we can even perfrom coarse localisation based on the feature representation for predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4K_txOPyd8_"},"outputs":[],"source":["heatmap_for_top_pred ('cat.00000.jpg', model)"]},{"cell_type":"markdown","metadata":{"id":"LCqObEvkyd9A"},"source":["Here the top prediction is not what we're looking for so let's have a look at the feature map for specific class predictions."]},{"cell_type":"markdown","metadata":{"id":"nWtz1xWhyd9A"},"source":["#### Select the class you want to show\n","As you can see the top prediction is not a cat so let's take a look inside and select the cat class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ot70oLr-yd9A"},"outputs":[],"source":["analysed_preds = 7\n","decode_preds[0][analysed_preds][1]"]},{"cell_type":"markdown","metadata":{"id":"pxMUTNBiyd9B"},"source":["#### Generate a heat map from specific layer\n","\n","First, inspect the model to look for the layer names:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIqcnv9zyd9B"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRvo6tFlyd9C"},"outputs":[],"source":["#todo\n","extract_layer_name = 'conv_pw_13'  #'conv_pw_2' # 'conv_pw_4' #'conv_pw_12'  #'conv_pw_13'"]},{"cell_type":"code","source":["img2infer = 'cat.00000.jpg'\n","img = load_img(img2infer, target_size=(224, 224))\n","x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","# mark the convolution layer for feature extraction\n","layer_index = extract_layer_name\n","model = Model(inputs=base_model.input, outputs=(base_model.get_layer(layer_index).output,base_model.layers[-1].output))\n","conv_out, preds = model.predict(x)\n","\n","top13_preds = preds[0].argsort()[-13:][::-1]\n","analyzed_class = top13_preds[analysed_preds]\n","pred_index = analyzed_class\n","\n","with tf.GradientTape() as tape:\n","    last_conv_layer_output, preds = model(x)\n","    if pred_index is None:\n","        pred_index = tf.argmax(preds[0])\n","    class_channel = preds[:, pred_index]\n","\n","grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","last_conv_layer_output = last_conv_layer_output[0]\n","heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","heatmap = tf.squeeze(heatmap)\n","\n","heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","heatmap = heatmap.numpy()\n","\n","print('Heatmap calculated')\n"],"metadata":{"id":"qjSxdo_e47Nf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j5aXJaz5yd9D"},"source":["#### Plot heatmap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKMVFTiKyd9D"},"outputs":[],"source":["fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 15))\n","axes[0].matshow(heatmap)\n","axes[0].set_title('Heat map based on layer: '+extract_layer_name)\n","\n","\n","axes[1].imshow(img, alpha=0.7)\n","\n","heatmap = cv2.resize(heatmap, (224,224))\n","heatmap = np.uint8(255 * heatmap)\n","axes[1].imshow(heatmap, cmap='jet', alpha=0.3)\n","\n","axes[1].set_title('Heat map for prediction: '+str(decode_preds[0][analysed_preds][1]));\n"]},{"cell_type":"markdown","metadata":{"id":"GYtZU66Byd9D"},"source":["#### Plot bounding box"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvDUK3gnyd9E"},"outputs":[],"source":["# Plot BoundingBox\n","fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 15))\n","axes.imshow(img, alpha=0.7)\n","\n","axes.imshow(heatmap, cmap='jet', alpha=0.3)\n","\n","left, up, down, right = get_bounds(heatmap, percentile=95)\n","\n","rect = patches.Rectangle((left, up), (right-left), (down-up), linewidth=1,  edgecolor='r', facecolor='none')\n","\n","axes.add_patch(rect)\n","axes.set_title('Heat map and bounding box for prediction: '+str(decode_preds[0][analysed_preds][1]));\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2_M4LDjwyd9E"},"source":["## Summary\n","Pretrained networks are a good way to start your project as they are usually trained on large amounts of data and using resources that arenâ€™t usually available to everyone. Based on the feature maps inside CNN you can see the focus (attention) of the network as well as perform coarse localisation."]},{"cell_type":"markdown","metadata":{"id":"f_3hIvs7yd9F"},"source":["## TL;DR\n","Network trained for classification + Class Activation Maps (CAMs) -> Weakly Supervised Object Localisation (WSOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tvvuyzumyd9F"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}