{"cells":[{"cell_type":"markdown","metadata":{"id":"mNNKSYULyd8r"},"source":["# Image classification wth CNN pretrained on imagenet database"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE0xXr5xyd8x"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import matplotlib.patches as patches\n","\n","from keras import backend as keras\n","from keras.layers import *\n","from keras.models import Model\n","\n","from keras.applications.mobilenet import MobileNet\n","from keras.applications.mobilenet import preprocess_input, decode_predictions\n","\n","#from keras.preprocessing import image\n","#from keras.preprocessing.image import load_img\n","from tensorflow.keras.utils import load_img\n","from tensorflow.keras.utils import img_to_array\n","\n","\n","from keras import backend as K\n","import tensorflow as tf\n","\n","#from CNN_utils import *"]},{"cell_type":"code","source":["#!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/CNN_utils.py' -O CNN_utils.py\n","\n","!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/cat.1.jpg' -O cat.1.jpg\n","!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/cat.0000.jpg' -O cat.0000.jpg\n","!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/cat.00000.jpg' -O cat.00000.jpg\n","!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/dog.1018.jpg' -O dog.1018.jpg\n"],"metadata":{"id":"4J2n-j67y3J2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget 'https://raw.githubusercontent.com/knave88/pretrainedCNN_DEMO/main/CNN_utils.py' -O CNN_utils.py\n","from CNN_utils import *"],"metadata":{"id":"I3j9TTKunZ6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAH4_WgYyd8z"},"outputs":[],"source":["plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger"]},{"cell_type":"markdown","metadata":{"id":"5qRj0M6Yyd80"},"source":["## Load pretrained CNN\n","We'll use Moblilenet as it is one of the lightweight models. Depthwise Separable Convolution is used to reduce the model size and complexity.\n","We load a MobileNet model, with weights pre-trained on ImageNet. The default input size for this model is 224x224. The network outputs the probability for 1000 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Z6cc3ufryd81"},"outputs":[],"source":["base_model = MobileNet(include_top=True, weights='imagenet') #classes=1000\n","#base_model.summary()\n","print(\"Total parameters: \"+str(base_model.count_params())+\" in \"+str(len(base_model.layers))+\" layers\" )"]},{"cell_type":"markdown","metadata":{"id":"LtNeHuPOyd81"},"source":["<img src=\"https://github.com/knave88/pretrainedCNN_DEMO/blob/main/mobilenet.png?raw=1\">"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"YBa4FGBJyd82"},"outputs":[],"source":["# mark the convolution layer for feature extraction\n","layer_index = 'conv_pw_13'\n","model = Model(inputs=base_model.input, outputs=(base_model.get_layer(layer_index).output,base_model.layers[-1].output))"]},{"cell_type":"markdown","metadata":{"id":"9n11DBWkyd83"},"source":["## Load cat image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntmEOYAByd83"},"outputs":[],"source":["img_path = 'cat.1.jpg'\n","img = load_img(img_path, target_size=(224, 224))\n","\n","plt.figure()\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"nlWrFAe2yd84"},"source":["## Inference\n","We set the image above as input to the network and get predictions from the output layer of the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13D0ZQ2Fyd85"},"outputs":[],"source":["x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","conv_out, preds = model.predict(x)\n","# decode the results into a list of tuples (class, description, probability)\n","# (one such list for each sample in the batch)\n","decode_preds = decode_predictions(preds, top=13)\n","print('Top 3 predictions:', decode_preds[0][0][1], decode_preds[0][1][1], decode_preds[0][2][1])\n"]},{"cell_type":"markdown","metadata":{"id":"qkTRal_gyd86"},"source":["## Load dog image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMLbsKwRyd86"},"outputs":[],"source":["img_path = 'dog.1018.jpg'\n","img = load_img(img_path, target_size=(224, 224))\n","\n","plt.figure()\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"R2kOmXj5yd87"},"source":["## Inference\n","We set the image above as input to the network and get predictions from the output layer of the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-rZgnBFyd87"},"outputs":[],"source":["x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","conv_out, preds = model.predict(x)\n","# decode the results into a list of tuples (class, description, probability)\n","# (one such list for each sample in the batch)\n","decode_preds = decode_predictions(preds, top=13)\n","print('Top 3 predictions:', decode_preds[0][0][1], decode_preds[0][1][1], decode_preds[0][2][1])\n"]},{"cell_type":"markdown","metadata":{"id":"w-2mWXI4yd88"},"source":["## More sophisticated example"]},{"cell_type":"markdown","metadata":{"id":"EKSZc_Qsyd88"},"source":["First, we'll begin with simple enough image of a cat."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9ZR2Hfjyd88"},"outputs":[],"source":["img_path = 'cat.0000.jpg'\n","img = load_img(img_path, target_size=(224, 224))\n","x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","conv_out, preds = model.predict(x)\n","decode_preds = decode_predictions(preds, top=13)\n","\n","plt.figure()\n","plt.imshow(img)\n","print('Top prediction: '+str(decode_preds[0][0][1]));"]},{"cell_type":"markdown","metadata":{"id":"w1YOMQITyd89"},"source":["But what if the animal was not the only object present in the image?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pvJLzgZ2yd89"},"outputs":[],"source":["img_path = 'cat.00000.jpg'\n","img = load_img(img_path, target_size=(224, 224))\n","\n","plt.figure()\n","plt.imshow(img)\n","\n","x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","conv_out, preds = model.predict(x)\n","# decode the results into a list of tuples (class, description, probability)\n","# (one such list for each sample in the batch)\n","decode_preds = decode_predictions(preds, top=13)\n","print('Top 3 predictions:', decode_preds[0][0][1], decode_preds[0][1][1], decode_preds[0][2][1])"]},{"cell_type":"markdown","metadata":{"id":"KYVvhfyuyd89"},"source":["As the cat is not in the top 3, let's see further down the list of top predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UM07tarfyd8-"},"outputs":[],"source":["decode_preds[0]"]},{"cell_type":"markdown","metadata":{"id":"GmJPpOYZyd8-"},"source":["There it is: No 8 'Egyptian_cat' class with probability of 0.014584725.\n","\n","Since this network has 1000 classes it focuses on other objects present in the image."]},{"cell_type":"markdown","metadata":{"id":"D0Mbyx0nyd8-"},"source":["# Weakly supervised object localisation\n","\n","Weakly-supervised learning provides a way to omit difficult process of pixel-level annotation of data. Models trained through WSL exploit unlabeled inputs, as well as coarse or ambiguous labels.\n","\n","The goal of such approach is to classify images with only global image-level labels, while producing pixel-level label predictions, thereby localizing the important regions of interest that are linked to the model's global decision. Pinpointing image sub-regions that were used by the model to make its global image-class prediction not only provides weakly supervised segmentation, but also enables interpretable deep-network classifiers."]},{"cell_type":"markdown","metadata":{"id":"Y_j0Pak4yd8-"},"source":["## Heat map for top predictions\n","Let's take a look at the feature map from last convoluton layer to see the area on which the prediction was made."]},{"cell_type":"code","source":["def get_bounds(out, percentile=95):\n","    # Get bounding box of 95+ percentile pixels\n","    a = out.flatten()\n","    filtered = np.array([1 if x > np.percentile(a, percentile) else 0 for x in a]).reshape(224,224)\n","    left, up, down, right = 224, 224, 0, 0\n","    for x in range(224):\n","        for y in range(224):\n","            if filtered[y,x] == 1:\n","                left = min(left, x)\n","                right = max(right, x)\n","                up = min(up, y)\n","                down = max(down, y)\n","    return left, up, down, right\n","\n","def heatmap_for_top_pred(img2infer, model_CAM,  pred_index=None, figsizeX=12, analysed_preds=7):\n","    img = load_img(img2infer, target_size=(224, 224))\n","    x = img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = model_CAM(x)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","\n","    heatmap = cv2.resize(heatmap.numpy(), (224,224))\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    left, up, down, right = get_bounds(heatmap, percentile=95)\n","\n","    rect = patches.Rectangle((left, up), (right-left), (down-up), linewidth=1,  edgecolor='r', facecolor='none')\n","\n","    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(figsizeX, figsizeX))\n","    axes.imshow(img, alpha=0.7)\n","    axes.imshow(heatmap, cmap='jet', alpha=0.3)\n","    left, up, down, right = get_bounds(heatmap, percentile=95)\n","    rect = patches.Rectangle((left, up), (right-left), (down-up), linewidth=1,  edgecolor='r', facecolor='none')\n","    axes.add_patch(rect)\n","    axes.set_title('Heat map and bounding box for prediction: '+str(decode_preds[0][analysed_preds][1]))\n","    return None"],"metadata":{"id":"ORQDK5M91BTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sprJNKb-yd8_"},"outputs":[],"source":["heatmap_for_top_pred ('cat.1.jpg', model) #extract_layer_name='conv_pw_13'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVhf9-Wlyd8_"},"outputs":[],"source":["heatmap_for_top_pred ('dog.1018.jpg', model)"]},{"cell_type":"markdown","metadata":{"id":"-Ldysxhzyd8_"},"source":["Here we can see based on which part of image the predictions were made.\n","As you can see we can even perfrom coarse localisation based on the feature representation for predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4K_txOPyd8_"},"outputs":[],"source":["heatmap_for_top_pred ('cat.00000.jpg', model)"]},{"cell_type":"markdown","metadata":{"id":"LCqObEvkyd9A"},"source":["Here the top prediction is not what we're looking for so let's have a look at the feature map for specific class predictions."]},{"cell_type":"markdown","metadata":{"id":"nWtz1xWhyd9A"},"source":["#### Select the class you want to show\n","As you can see the top prediction is not a cat so let's take a look inside and select the cat class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ot70oLr-yd9A"},"outputs":[],"source":["analysed_preds = 7\n","decode_preds[0][analysed_preds][1]"]},{"cell_type":"markdown","metadata":{"id":"pxMUTNBiyd9B"},"source":["#### Generate a heat map from specific layer\n","\n","First, inspect the model to look for the layer names:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIqcnv9zyd9B"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRvo6tFlyd9C"},"outputs":[],"source":["#todo\n","extract_layer_name = 'conv_pw_13'  #'conv_pw_2' # 'conv_pw_4' #'conv_pw_12'  #'conv_pw_13'"]},{"cell_type":"code","source":["img2infer = 'cat.00000.jpg'\n","img = load_img(img2infer, target_size=(224, 224))\n","x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","# mark the convolution layer for feature extraction\n","layer_index = extract_layer_name\n","model = Model(inputs=base_model.input, outputs=(base_model.get_layer(layer_index).output,base_model.layers[-1].output))\n","conv_out, preds = model.predict(x)\n","\n","top13_preds = preds[0].argsort()[-13:][::-1]\n","analyzed_class = top13_preds[analysed_preds]\n","pred_index = analyzed_class\n","\n","with tf.GradientTape() as tape:\n","    last_conv_layer_output, preds = model(x)\n","    if pred_index is None:\n","        pred_index = tf.argmax(preds[0])\n","    class_channel = preds[:, pred_index]\n","\n","grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","last_conv_layer_output = last_conv_layer_output[0]\n","heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","heatmap = tf.squeeze(heatmap)\n","\n","heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","heatmap = heatmap.numpy()\n","\n","print('Heatmap calculated')\n"],"metadata":{"id":"qjSxdo_e47Nf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j5aXJaz5yd9D"},"source":["#### Plot heatmap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKMVFTiKyd9D"},"outputs":[],"source":["fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 15))\n","axes[0].matshow(heatmap)\n","axes[0].set_title('Heat map based on layer: '+extract_layer_name)\n","\n","\n","axes[1].imshow(img, alpha=0.7)\n","\n","heatmap = cv2.resize(heatmap, (224,224))\n","heatmap = np.uint8(255 * heatmap)\n","axes[1].imshow(heatmap, cmap='jet', alpha=0.3)\n","\n","axes[1].set_title('Heat map for prediction: '+str(decode_preds[0][analysed_preds][1]));\n"]},{"cell_type":"markdown","metadata":{"id":"GYtZU66Byd9D"},"source":["#### Plot bounding box"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvDUK3gnyd9E"},"outputs":[],"source":["# Plot BoundingBox\n","fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 15))\n","axes.imshow(img, alpha=0.7)\n","\n","axes.imshow(heatmap, cmap='jet', alpha=0.3)\n","\n","left, up, down, right = get_bounds(heatmap, percentile=95)\n","\n","rect = patches.Rectangle((left, up), (right-left), (down-up), linewidth=1,  edgecolor='r', facecolor='none')\n","\n","axes.add_patch(rect)\n","axes.set_title('Heat map and bounding box for prediction: '+str(decode_preds[0][analysed_preds][1]));\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2_M4LDjwyd9E"},"source":["## Summary\n","Pretrained networks are a good way to start your project as they are usually trained on large amounts of data and using resources that aren’t usually available to everyone. Based on the feature maps inside CNN you can see the focus (attention) of the network as well as perform coarse localisation."]},{"cell_type":"markdown","metadata":{"id":"f_3hIvs7yd9F"},"source":["## TL;DR\n","Network trained for classification + Class Activation Maps (CAMs) -> Weakly Supervised Object Localisation (WSOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tvvuyzumyd9F"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}